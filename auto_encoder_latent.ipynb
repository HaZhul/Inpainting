{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 SUPER'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import comet_ml\n",
    "from config import API_KEY\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import einops\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dominik\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import v2\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "def preprocess(example):\n",
    "    example['image'] = example['image'].float() / 255.0\n",
    "    example['image'] = transform(example['image'])\n",
    "    return example\n",
    "\n",
    "\n",
    "ds = load_dataset(\"Artificio/WikiArt_Full\").with_format('torch')\n",
    "\n",
    "train_test_split = ds[\"train\"].train_test_split(test_size=0.15)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "ds = {\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "ds = DatasetDict(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        ds['train'],\n",
    "        batch_size=80,\n",
    "        num_workers=12,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        ds['test'],\n",
    "        batch_size=80,\n",
    "        num_workers=12,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).cuda()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        ssim_score = self.ssim(x, y)\n",
    "        return 1 - ssim_score\n",
    "    \n",
    "\n",
    "def log_images_to_comet(images, experiment, idx, epoch, tr_step):\n",
    "    grid = make_grid(images, nrow=8, padding=2)\n",
    "    \n",
    "    np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(np.clip(np_grid, 0, 1))\n",
    "    plt.axis(\"off\")\n",
    "    experiment.log_figure(figure_name=f'{tr_step}:{idx}', figure=plt, step=epoch)\n",
    "    plt.close()\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import models\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=1.0, perceptual_weight=0.05, style_weight=0.0001):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        vgg = vgg19(pretrained=True).features\n",
    "        self.vgg_layers = vgg[:36].eval()      \n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def compute_feature_difference(self, prediction, target):\n",
    "        feature_diff = 0\n",
    "        for p_f, t_f in zip(prediction, target):\n",
    "            p_f_norm = F.normalize(p_f, p=2, dim=1)\n",
    "            t_f_norm = F.normalize(t_f, p=2, dim=1)\n",
    "            feature_diff += torch.norm(p_f_norm - t_f_norm, p=2)\n",
    "\n",
    "        return feature_diff\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            x = layer(x)\n",
    "            if i in {3, 8, 17, 26, 35}:  # conv1_2, conv2_2, conv3_4, conv4_4, conv5_4\n",
    "                features.append(x)\n",
    "        return features\n",
    "    \n",
    "    def forward(self, prediction, target):\n",
    "        self.vgg_layers.to(dtype=prediction.dtype, device=prediction.device)\n",
    "\n",
    "        prediction_features = self.extract_features(prediction)\n",
    "        target_features = self.extract_features(target)\n",
    "\n",
    "        # L1 Loss\n",
    "        l1_loss = self.l1_loss(prediction, target)\n",
    "\n",
    "        # Perceptual Loss (Feature-Level MSE)\n",
    "        perceptual_loss = sum(F.mse_loss(p_f, t_f)\n",
    "                              for p_f, t_f in zip(prediction_features, target_features))\n",
    "\n",
    "        # Style Loss (Gram Matrix MSE)\n",
    "        style_loss = self.compute_feature_difference(prediction_features, target_features)\n",
    "\n",
    "\n",
    "        l1 = self.l1_weight * l1_loss\n",
    "        perceptual = self.perceptual_weight * perceptual_loss\n",
    "        style = self.style_weight * style_loss\n",
    "\n",
    "        return l1, perceptual, style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riba\\AppData\\Local\\Temp\\ipykernel_2604\\3308278937.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/foxtold/un-latent/4683bdd15fd64b889cc6e7de4e0c3b40\n",
      "\n",
      "e:\\repos\\inpainting-art\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\repos\\inpainting-art\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "TRAIN_0: 1098it [09:14,  1.98it/s]\n",
      "TEST_0: 194it [00:49,  3.89it/s]\n",
      "TRAIN_1: 1098it [08:40,  2.11it/s]\n",
      "TEST_1: 194it [00:47,  4.09it/s]\n",
      "TRAIN_2: 1098it [08:53,  2.06it/s]\n",
      "TEST_2: 194it [00:47,  4.05it/s]\n",
      "TRAIN_3: 1098it [08:55,  2.05it/s]\n",
      "TEST_3: 194it [00:49,  3.94it/s]\n",
      "TRAIN_4: 1098it [08:54,  2.05it/s]\n",
      "TEST_4: 194it [00:50,  3.86it/s]\n",
      "TRAIN_5: 1098it [09:01,  2.03it/s]\n",
      "TEST_5: 194it [00:46,  5.28it/s]\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to log system metrics: [sys.ram,sys.cpu,sys.load]\n",
      "TEST_5: 194it [00:47,  4.07it/s]\n",
      "TRAIN_6: 1098it [08:36,  2.13it/s]\n",
      "TEST_6: 194it [00:47,  4.07it/s]\n",
      "TRAIN_7: 1098it [08:46,  2.09it/s]\n",
      "TEST_7: 194it [00:49,  3.94it/s]\n",
      "TRAIN_8: 1098it [09:05,  2.01it/s]\n",
      "TEST_8: 194it [00:49,  3.93it/s]\n",
      "TRAIN_9: 1098it [08:40,  2.11it/s]\n",
      "TEST_9: 194it [00:48,  4.02it/s]\n",
      "TRAIN_10: 195it [01:33,  2.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 64\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# ssim_loss = ssim_loss_fn(predictions, images)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# mse_loss = mse_loss_fn(predictions, images)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# loss = lambda_ssim * ssim_loss + lambda_mse * mse_loss\u001b[39;00m\n\u001b[0;32m     63\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 64\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# comet_experiment.log_metric('ssim_loss', ssim_loss.item())\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# comet_experiment.log_metric('mse_loss', mse_loss.item())\u001b[39;00m\n",
      "File \u001b[1;32me:\\repos\\inpainting-art\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32me:\\repos\\inpainting-art\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32me:\\repos\\inpainting-art\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchinfo import summary\n",
    "\n",
    "from models import Encoder, Decoder, AutoEncoder\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    lambda_ssim = 0.2\n",
    "    lambda_mse = 0.8\n",
    "\n",
    "\n",
    "    latent_width = 1024\n",
    "    scaler = GradScaler()\n",
    "    encoder = Encoder(latent_width)\n",
    "    decoder = Decoder(latent_width)\n",
    "    model = AutoEncoder(encoder=encoder, decoder=decoder)\n",
    "    model.apply(init_weights)\n",
    "    model.cuda()\n",
    "\n",
    "    comet_experiment = comet_ml.Experiment(api_key=API_KEY, project_name='UN_latent')\n",
    "    comet_experiment.log_parameters(\n",
    "        {\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'train_size': ds['train'].num_rows,\n",
    "            'test_size': ds['test'].num_rows,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    summ = summary(model, (1,3,256,256), device='cuda',depth=5)\n",
    "    comet_experiment.set_model_graph(f'{model.__repr__()}\\n{summ}')\n",
    "\n",
    "    num_epochs = 50\n",
    "\n",
    "    loss_fn = CombinedLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    comet_experiment.log_parameter('num_epochs', num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        comet_experiment.set_epoch(epoch)\n",
    "                    \n",
    "\n",
    "        model.train()\n",
    "        with comet_experiment.train() as train:\n",
    "            for idx, batch in tqdm(enumerate(train_loader), desc=f'TRAIN_{epoch}'):\n",
    "                comet_experiment.set_step(idx + epoch * len(train_loader))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                images = batch['image'] / 255.0\n",
    "                labels = batch['image']\n",
    "                images = images.cuda()\n",
    "                with torch.autocast(device_type='cuda'):\n",
    "                    predictions, latents = model(images)\n",
    "                    l1, per, style = loss_fn(predictions, images)\n",
    "                    loss = l1 + per + style\n",
    "                scaler.scale(loss.float()).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "\n",
    "                comet_experiment.log_metric('l1_loss', l1)\n",
    "                comet_experiment.log_metric('per_loss', per)\n",
    "                comet_experiment.log_metric('style_loss', style)\n",
    "        \n",
    "        model.eval()\n",
    "        with comet_experiment.validate() as test, torch.no_grad() as nograd:\n",
    "            for idx, batch in tqdm(enumerate(test_loader), desc=f'TEST_{epoch}'):\n",
    "                comet_experiment.set_step(idx + epoch * len(test_loader))\n",
    "\n",
    "                images = batch['image'] / 255.0\n",
    "                images = images.cuda()\n",
    "                with torch.autocast(device_type='cuda'):\n",
    "                    predictions, latents = model(images)\n",
    "                    l1, per, style = loss_fn(predictions, images)\n",
    "                    loss = l1 + per + style\n",
    "\n",
    "                comet_experiment.log_metric('l1_loss', l1)\n",
    "                comet_experiment.log_metric('per_loss', per)\n",
    "                comet_experiment.log_metric('style_loss', style)\n",
    "\n",
    "                if idx < 2:\n",
    "                    concatenated = torch.cat([images, predictions], dim=3).cpu()\n",
    "                    log_images_to_comet(concatenated, comet_experiment, idx, epoch, 'TEST')\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(encoder.state_dict(), f\"models/encoder_{epoch}.pth\")\n",
    "            torch.save(decoder.state_dict(), f\"models/decoder_{epoch}.pth\")\n",
    "            torch.save(model.state_dict(), f\"models/model_{epoch}.pth\")\n",
    "\n",
    "\n",
    "    log_model(comet_experiment, model, model_name=\"AutoEncoder\")\n",
    "    comet_experiment.end()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
